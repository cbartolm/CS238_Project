{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA228/CS238 Optional Final Project: Escape Roomba\n",
    "\n",
    "## QMPD + MCTS + Heuristic Proportional Controller using the Bumper Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# activate project environment\n",
    "# include these lines of code in any future scripts/notebooks\n",
    "#---\n",
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"AA228FinalProject\")\n",
    "    jenv = joinpath(dirname(@__FILE__()), \".\") # this assumes the notebook is in the same dir\n",
    "    # as the Project.toml file, which should be in top level dir of the project. \n",
    "    # Change accordingly if this is not the case.\n",
    "    Pkg.activate(jenv)\n",
    "end\n",
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "using AA228FinalProject\n",
    "using POMDPs\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using ParticleFilters\n",
    "using POMDPSimulators\n",
    "using Cairo\n",
    "using Gtk\n",
    "using Random\n",
    "using Printf\n",
    "\n",
    "# new libraries\n",
    "using JLD # alpha vectors\n",
    "using BasicPOMCP\n",
    "using Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sensor and construct POMDP\n",
    "\n",
    "### I. The POMDP\n",
    "\n",
    "In this notebook, we are going to try an offline approximation method (QMPD from subsection 6.4 in the course reader). Offline POMDP solution methods involve performing all or most of the computation prior to execution. We are going to compute one alpha vector per action. However, in order to use this technique we need to discretize the state and action spaces. Let's do this in the following subsection.\n",
    "\n",
    "##### a) Definition of state space and action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### i) State Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define granularity of the discretization\n",
    "linspace_x = 20\n",
    "linspace_y = 20\n",
    "linspace_th = 10\n",
    "\n",
    "# Create the State Space \n",
    "discretized_state_space = DiscreteRoombaStateSpace(linspace_x,linspace_y,linspace_th)\n",
    "\n",
    "print(discretized_state_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ii) Action Space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Angles\n",
    "om_list = [-1, 0, 1]\n",
    "\n",
    "# Velocities\n",
    "v_list = [0, 5, 10]\n",
    "\n",
    "discretized_action_space = vec(collect(RoombaAct(v, om) for v in v_list, om in om_list))\n",
    "\n",
    "print(discretized_action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Define Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor = Bumper()\n",
    "config = 1 # 1,2, or 3\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config, sspace=discretized_state_space,\n",
    "        aspace=discretized_action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Particle Filter\n",
    "\n",
    "First, we instantiate a resampler, which is responsible for updating the belief state given an observation. The first argument for both resamplers is the number of particles that represent the belief state. The lidar resampler takes a low-variance resampler as an additional argument, which is responsible for efficiently resampling a weighted set of particles. \n",
    "\n",
    "Next, we instantiate a ```SimpleParticleFilter```, which enables us to perform our belief updates.\n",
    "\n",
    "Finally, we pass this particle filter into a custom struct called a ```RoombaParticleFilter```, which takes two additional arguments. These arguments specify the noise in the velocity and turn-rate, used when propegating particles according to the action taken. These can be tuned depending on the type of sensor used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_particles = 500\n",
    "resampler = BumperResampler(num_particles)\n",
    "\n",
    "spf = SimpleParticleFilter(m, resampler)\n",
    "\n",
    "v_noise_coefficient = 2.0\n",
    "om_noise_coefficient = 0.5\n",
    "\n",
    "belief_updater = RoombaParticleFilter(spf, v_noise_coefficient, om_noise_coefficient);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the POMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Search Tree + QMPD + Heuristic Proportional Controller\n",
    "\n",
    "For this final approach, we are going to use MCTS + QMPD when there is too much uncertainty in the belief or the Heuristic Controller provided in the first notebook (a proportional controller) when there is enough evidence as it will send Roomba directly to the target. \n",
    "\n",
    "Next, we define a function that can take in our policy and the belief state and return the desired action. We do this by defining a new ```POMDPs.action``` function that will work with our policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An heuristic controller can be designed as we know the target coordinates\n",
    "goal_xy = get_goal_xy(m)\n",
    "\n",
    "## First Struct\n",
    "# Use alphas to get started\n",
    "struct initialHeuristic <: Policy\n",
    "    alphas::Array{Array{Float64,1},1} \n",
    "end\n",
    "\n",
    "## First Function\n",
    "function POMDPs.action(p::initialHeuristic, s::RoombaState)\n",
    "    ## Arguments: Policy struct + State\n",
    "    ## Classic QMPD\n",
    "    j = stateindex(m, s)\n",
    "    idx = findmax([p.alphas[i][j] for i in 1:length(actions(m))])[2]\n",
    "    \n",
    "    # Get action in state s with highest alpha vector \n",
    "    action = actions(m)[idx]\n",
    "    return action\n",
    "end\n",
    "\n",
    "######## Execution ########\n",
    "\n",
    "# Load alphas\n",
    "alphas = load(\"QMDP_alphas.jld\")[\"QMDP_alphas\"]\n",
    "\n",
    "# QMDP heuristic \n",
    "p = initialHeuristic(alphas)\n",
    "\n",
    "# MC initialized with heuristic\n",
    "solver = POMCPSolver(estimate_value=FORollout(p))\n",
    "\n",
    "# Get policy\n",
    "control_policy = solve(solver, m);\n",
    "\n",
    "## Second Struct\n",
    "# Policy struct to update with heuristic controller \n",
    "mutable struct heuristicPolicy <: Policy\n",
    "    control_policy \n",
    "end\n",
    "\n",
    "\n",
    "## Second Function\n",
    "function POMDPs.action(p::heuristicPolicy, b::ParticleCollection{RoombaState})\n",
    "    # Arguments: Policy Struct + Belief\n",
    "    \n",
    "    max_variance = 0.5 #Maximum allowable uncertainty to use the proportional controller\n",
    "    # Hyperparameter that defines the minimum acceptable level of evidence to use \n",
    "    # the proportional controller\n",
    "    K = 2 # Constant Proportional Controller \n",
    "    \n",
    "    v = 1 # Speed \n",
    "    n_x, n_y, n_theta = 25., 15., 5. \n",
    "    \n",
    "    # normalization\n",
    "    std = std(particles(b))[1:3] ./ [n_x, n_y, n_theta] \n",
    "    println(\"Variance :\", std, sum(std))\n",
    "    \n",
    "    # ask whether there is high confidence of the state\n",
    "    if sum(std) < max_variance && !AA228FinalProject.wall_contact(m, particle(b, 1))\n",
    "        println(\"Heuristic Proportional Controller\")\n",
    "        \n",
    "        ## From Notebook provided in the official Repo: \n",
    "        # use a proportional controller to navigate directly to the goal, using the mean belief state\n",
    "        # compute mean belief of a subset of particles\n",
    "        s = mean(b)\n",
    "\n",
    "        # compute the difference between our current heading and one that would\n",
    "        # point to the goal\n",
    "        goal_x, goal_y = goal_xy\n",
    "        x,y,th = s[1:3]\n",
    "        ang_to_goal = atan(goal_y - y, goal_x - x)\n",
    "        del_angle = wrap_to_pi(ang_to_goal - th)\n",
    "\n",
    "        # apply proportional control to compute the turn-rate\n",
    "        om = Kprop * del_angle        \n",
    "        \n",
    "        return RoombaAct(v, om)\n",
    "    end\n",
    "        \n",
    "    # If we are not confident enough --> Use MCTS + QMPD\n",
    "    best_action = action(p.control_policy, b)\n",
    "    return best_action\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation and rendering\n",
    "\n",
    "Here, we will demonstrate how to seed the environment, run a simulation, and render the simulation. To render the simulation, we use the ```Gtk``` package. \n",
    "\n",
    "The simulation is carried out using the ```stepthrough``` function defined in the package ```POMDPSimulators.jl```. During a simulation, a window will open that renders the scene. It may be hidden behind other windows on your desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first seed the environment\n",
    "Random.seed!(1)\n",
    "\n",
    "# run the simulation\n",
    "c = @GtkCanvas()\n",
    "win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "for (t, step) in enumerate(stepthrough(m, control_policy, belief_updater, max_steps=100))\n",
    "    @guarded draw(c) do widget\n",
    "        \n",
    "        # the following lines render the room, the particles, and the roomba\n",
    "        ctx = getgc(c)\n",
    "        set_source_rgb(ctx,1,1,1)\n",
    "        paint(ctx)\n",
    "        render(ctx, m, step)\n",
    "\n",
    "        std_pos = std(particles(step.b)) ./ [25., 15., 5., 1.] \n",
    "        \n",
    "        move_to(ctx,300,400)\n",
    "        show_text(ctx, @sprintf(\"t=%d, s=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "    end\n",
    "    show(c)\n",
    "    sleep(0.01) # to slow down the simulation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying initial states and beliefs\n",
    "If, for debugging purposes, you would like to hard-code a starting location or initial belief state for the roomba, you can do so by taking the following steps.\n",
    "\n",
    "First, we define the initial state using the following line of code:\n",
    "```\n",
    "is = RoombaState(x,y,th,0.)\n",
    "```\n",
    "Where ```x``` and ```y``` are the x,y coordinates of the starting location and ```th``` is the heading in radians. The last entry, ```0.```, respresents whether the state is terminal, and should remain unchanged.\n",
    "\n",
    "If you would like to initialize the Roomba's belief as perfectly localized, you can do so with the following line of code:\n",
    "```\n",
    "b0 = Deterministic(is)\n",
    "```\n",
    "If you would like to initialize the standard unlocalized belief, use these lines:\n",
    "```\n",
    "dist = initialstate_distribution(m)\n",
    "b0 = initialize_belief(belief_updater, dist)\n",
    "```\n",
    "Finally, we call the ```stepthrough``` function using the initial state and belief as follows:\n",
    "```\n",
    "stepthrough(m,planner,belief_updater,b0,is,max_steps=300)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "We intialize the robot using five different random seeds, and simulate its performance for 100 time-steps. We then sum the rewards experienced during its interaction with the environment and track this total reward for the five trials.\n",
    "Finally, we report the mean and standard error for the total reward. The standard error is the standard deviation of a sample set divided by the square root of the number of samples, and represents the uncertainty in the estimate of the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience: 1 ----- Reward: -10.0\n",
      "Experience: 2 ----- Reward: -10.0\n",
      "Experience: 3 ----- Reward: 8.5\n",
      "Experience: 4 ----- Reward: -10.0\n",
      "Experience: 5 ----- Reward: -10.0\n",
      "Experience: 6 ----- Reward: -10.0\n",
      "Experience: 7 ----- Reward: 9.4\n",
      "Experience: 8 ----- Reward: -10.0\n",
      "Experience: 9 ----- Reward: 5.8\n",
      "Experience: 10 ----- Reward: -10.0\n",
      "The total execution time of the 10 experiments is: 1459.4535930156708\n",
      "Mean Total Reward: -4.630, StdErr Total Reward: 3.887"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "start = time()\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "method = \"final_method\"\n",
    "df = DataFrame(num_experience = Int[], reward = String[])\n",
    "\n",
    "for exp = 1:10\n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = heuristicPolicy(control_policy)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m, control_policy, belief_updater, max_steps=100)])\n",
    "    \n",
    "    println(\"Experience: \", string(exp), \" ----- Reward: \", traj_rewards)\n",
    "    push!(total_rewards, traj_rewards)\n",
    "    \n",
    "    # Save in a dataframe to finally pull results in a .csv file\n",
    "    push!(df, (exp, string(traj_rewards)))\n",
    "    CSV.write(string(method, \".csv\"), df) \n",
    "end\n",
    "\n",
    "total_time = time() - start\n",
    "\n",
    "print(\"The total execution time of the 10 experiments is: \", total_time, \"\\n\")\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Takes around 30 minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "597e9454676940c7a10616e732377653",
   "lastKernelId": "8e4cb2f6-6ff9-4dc9-b95b-5e7dba722782"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
