{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA228/CS238 Optional Final Project: Escape Roomba\n",
    "\n",
    "## Baseline Model II: Bumper Sensor Heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/cbartolm/Desktop/Projects/CS238_Project/Project.toml\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate project environment\n",
    "# include these lines of code in any future scripts/notebooks\n",
    "#---\n",
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"AA228FinalProject\")\n",
    "    jenv = joinpath(dirname(@__FILE__()), \".\") # this assumes the notebook is in the same dir\n",
    "    # as the Project.toml file, which should be in top level dir of the project. \n",
    "    # Change accordingly if this is not the case.\n",
    "    Pkg.activate(jenv)\n",
    "end\n",
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/cbartolm/.julia/compiled/v1.0/AA228FinalProject/uFJfC.ji for AA228FinalProject [fe2df5ea-4d44-4e5a-a895-9dbc87b19b37]\n",
      "└ @ Base loading.jl:1187\n",
      "┌ Info: Recompiling stale cache file /Users/cbartolm/.julia/compiled/v1.0/POMDPSimulators/i1HOp.ji for POMDPSimulators [e0d0a172-29c6-5d4e-96d0-f262df5d01fd]\n",
      "└ @ Base loading.jl:1187\n"
     ]
    }
   ],
   "source": [
    "# import necessary packages\n",
    "using AA228FinalProject\n",
    "using POMDPs\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using ParticleFilters\n",
    "using POMDPSimulators\n",
    "using Cairo\n",
    "using Statistics\n",
    "using Printf\n",
    "using Gtk\n",
    "using Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sensor and construct POMDP\n",
    "\n",
    "In the following cell, we first instantiate a Bump sensor. The Bumper indicates when contact has been made between any part of the Roomba and any wall.\n",
    "\n",
    "Next, we instantiate the MDP, which defines the underlying simulation environment, assuming full observability. The MDP takes many arguments to specify details of the problem. One argument we must specify here is the ```config```. This argument, which can take values 1,2, or 3, specifies the room configuration, with each configuration corresponding to a different location for the goal and stairs. \n",
    "\n",
    "We are going to use the three different configs to see how well the policy does in three different environments. \n",
    "\n",
    "Finally, we instantiate the POMDP. The POMDP takes as an argument the underlying MDP as well as the sensor, which it uses to define the observation model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor = Bumper()\n",
    "config = 1 # 1,2, or 3\n",
    "\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Particle Filter\n",
    "\n",
    "First, we instantiate a resampler, which is responsible for updating the belief state given an observation. The first argument for both resamplers is the number of particles that represent the belief state. The lidar resampler takes a low-variance resampler as an additional argument, which is responsible for efficiently resampling a weighted set of particles. \n",
    "\n",
    "Next, we instantiate a ```SimpleParticleFilter```, which enables us to perform our belief updates.\n",
    "\n",
    "Finally, we pass this particle filter into a custom struct called a ```RoombaParticleFilter```, which takes two additional arguments. These arguments specify the noise in the velocity and turn-rate, used when propegating particles according to the action taken. These can be tuned depending on the type of sensor used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_particles = 4000\n",
    "resampler = BumperResampler(num_particles)\n",
    "\n",
    "spf = SimpleParticleFilter(m, resampler)\n",
    "\n",
    "v_noise_coefficient = 2.0\n",
    "om_noise_coefficient = 0.5\n",
    "\n",
    "belief_updater = RoombaParticleFilter(spf, v_noise_coefficient, om_noise_coefficient);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a policy : A heuristic based on the bumper sensor\n",
    "\n",
    "First we create a struct that subtypes the Policy abstract type, defined in the package ```POMDPPolicies.jl```. Here, we can also define certain parameters.\n",
    "\n",
    "Next, we define a function that can take in our policy and the belief state and return the desired action. We do this by defining a new ```POMDPs.action``` function that will work with our policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the policy to test\n",
    "mutable struct ToEnd <: Policy\n",
    "    num_translation_1::Int64 # Maximum number of translations between wall contacts 1 and 2\n",
    "    num_translation_2::Int64 # Maximum number of translations between wall contacts 2 and 3\n",
    "    num_rot_1::Int64 # Number rotations at the beginning\n",
    "    num_rot_2::Int64 # Number of rotations in first wall contact\n",
    "    num_collisions::Int64 # Number of collisions so far \n",
    "    # (Allows to define a piece-wise policy)\n",
    "end\n",
    "\n",
    "# define a new function that takes in the policy struct and current belief,\n",
    "# and returns the desired action\n",
    "function POMDPs.action(p::ToEnd, b::ParticleCollection{RoombaState})\n",
    "    \n",
    "    v = 4 # Same speed\n",
    "    \n",
    "    # Get goal coordinates\n",
    "    goal_xy = get_goal_xy(m)\n",
    "    goal_x, goal_y = goal_xy\n",
    "    \n",
    "     ## First action:\n",
    "     # Spin around to localize during the first 25 time-steps\n",
    "     ## Useless because we don't have a long range sensor ##\n",
    "    \n",
    "    if p.num_rot_1 < 25\n",
    "        p.num_rot_1 += 1\n",
    "        return RoombaAct(0.,1) \n",
    "    end\n",
    "    p.num_rot_1 += 1\n",
    "    \n",
    "    ## Second action:\n",
    "    # Move until you reach a wall\n",
    "    if p.num_collisions == 0 && !AA228FinalProject.wall_contact(m, particle(b, 1))\n",
    "        return RoombaAct(v, 0)\n",
    "    end\n",
    "    \n",
    "    # Update tracker that counts number of contacts in wall\n",
    "    if p.num_collisions == 0 && AA228FinalProject.wall_contact(m, particle(b, 1))\n",
    "        p.num_collisions = 1 \n",
    "    end\n",
    "    \n",
    "    ## Third action:\n",
    "    # Rotate several times in the first contact wall (max_number = 4)\n",
    "    if p.num_collisions == 1 && p.num_rot_2 <= 4\n",
    "        p.num_rot_2 += 1\n",
    "        return RoombaAct(0., 1)\n",
    "    end\n",
    "    \n",
    "    ## Fourth action:\n",
    "    # Allow 8 translations maximum\n",
    "    if p.num_collisions == 1 && p.num_rot_2 > 4 && p.num_translation_1 <= 8\n",
    "        p.num_translation_1 += 1\n",
    "        return RoombaAct(v, 0.)\n",
    "    end\n",
    "    \n",
    "    \n",
    "    if  p.num_collisions == 1 && p.num_translation_1 > 8\n",
    "        p.num_rot_2 += 1\n",
    "        p.num_collisions = 2 \n",
    "    end\n",
    "\n",
    "    if p.num_collisions == 2 && p.num_rot_2 <= 3\n",
    "        p.num_rot_2 += 1\n",
    "        return RoombaAct(0., -1)\n",
    "    end\n",
    "    \n",
    "    if p.num_collisions == 2 && p.num_rot_2 > 3 && !AA228FinalProject.wall_contact(m, particle(b, 1))\n",
    "        p.num_translation_2 += 1\n",
    "        return RoombaAct(v, 0.)\n",
    "    end  \n",
    "    \n",
    "    if p.num_collisions == 2 && AA228FinalProject.wall_contact(m, particle(b, 1))\n",
    "        p.num_rot_2 = 0\n",
    "        p.num_collisions = 3 \n",
    "    end\n",
    "    \n",
    "    if p.num_collisions == 3 && !AA228FinalProject.wall_contact(m, particle(b, 1))\n",
    "        return RoombaAct(v, 0.)\n",
    "    end\n",
    "    \n",
    "    if p.num_collisions == 3 && p.num_rot_2 <= 2 && AA228FinalProject.wall_contact(m, particle(b, 1))\n",
    "        p.num_rot_2 += 1\n",
    "        return RoombaAct(0., -1 -0.1)\n",
    "    end\n",
    "    \n",
    "    if p.num_collisions == 3 && p.num_rot_2 > 2\n",
    "        p.num_rot_2 = 0\n",
    "        return RoombaAct(v, 0.)\n",
    "    end   \n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation and rendering\n",
    "\n",
    "Here, we will demonstrate how to seed the environment, run a simulation, and render the simulation. To render the simulation, we use the ```Gtk``` package. \n",
    "\n",
    "The simulation is carried out using the ```stepthrough``` function defined in the package ```POMDPSimulators.jl```. During a simulation, a window will open that renders the scene. It may be hidden behind other windows on your desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first seed the environment\n",
    "Random.seed!(2)\n",
    "\n",
    "# reset the policy\n",
    "p = ToEnd(0,0,0,0,0) # here, the argument sets the time-steps elapsed to 0\n",
    "\n",
    "# run the simulation\n",
    "c = @GtkCanvas()\n",
    "win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "for (t, step) in enumerate(stepthrough(m, p, belief_updater, max_steps=150))\n",
    "    @guarded draw(c) do widget\n",
    "        \n",
    "        # the following lines render the room, the particles, and the roomba\n",
    "        ctx = getgc(c)\n",
    "        set_source_rgb(ctx,1,1,1)\n",
    "        paint(ctx)\n",
    "        render(ctx, m, step)\n",
    "        \n",
    "        # render some information that can help with debugging\n",
    "        # here, we render the time-step, the state, and the observation\n",
    "        move_to(ctx,300,400)\n",
    "        show_text(ctx, @sprintf(\"t=%d, state=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "    end\n",
    "    show(c)\n",
    "    sleep(0.1) # to slow down the simulation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying initial states and beliefs\n",
    "If, for debugging purposes, you would like to hard-code a starting location or initial belief state for the roomba, you can do so by taking the following steps.\n",
    "\n",
    "First, we define the initial state using the following line of code:\n",
    "```\n",
    "is = RoombaState(x,y,th,0.)\n",
    "```\n",
    "Where ```x``` and ```y``` are the x,y coordinates of the starting location and ```th``` is the heading in radians. The last entry, ```0.```, respresents whether the state is terminal, and should remain unchanged.\n",
    "\n",
    "If you would like to initialize the Roomba's belief as perfectly localized, you can do so with the following line of code:\n",
    "```\n",
    "b0 = Deterministic(is)\n",
    "```\n",
    "If you would like to initialize the standard unlocalized belief, use these lines:\n",
    "```\n",
    "dist = initialstate_distribution(m)\n",
    "b0 = initialize_belief(belief_updater, dist)\n",
    "```\n",
    "Finally, we call the ```stepthrough``` function using the initial state and belief as follows:\n",
    "```\n",
    "stepthrough(m,planner,belief_updater,b0,is,max_steps=300)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "Here, we demonstate a simple evaluation of the policy's performance for a few random seeds. This is meant to serve only as an example, and we encourage you to develop your own evaluation metrics.\n",
    "\n",
    "We intialize the robot using five different random seeds, and simulate its performance for 100 time-steps. We then sum the rewards experienced during its interaction with the environment and track this total reward for the five trials.\n",
    "Finally, we report the mean and standard error for the total reward. The standard error is the standard deviation of a sample set divided by the square root of the number of samples, and represents the uncertainty in the estimate of the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"DataFrames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/cbartolm/.julia/compiled/v1.0/CSV/HHBkp.ji for CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience: 1----- Reward: -10.400000000000004\n",
      "Experience: 2----- Reward: -1.7999999999999972\n",
      "Experience: 3----- Reward: -15.999999999999998\n",
      "Experience: 4----- Reward: -20.6\n",
      "Experience: 5----- Reward: -24.000000000000007\n",
      "Experience: 6----- Reward: -23.599999999999998\n",
      "Experience: 7----- Reward: -30.1\n",
      "Experience: 8----- Reward: -9.500000000000016\n",
      "Experience: 9----- Reward: -18.1\n",
      "Experience: 10----- Reward: -3.899999999999997\n",
      "Experience: 11----- Reward: -25.000000000000007\n",
      "Experience: 12----- Reward: -23.00000000000001\n",
      "Experience: 13----- Reward: -25.499999999999993\n",
      "Experience: 14----- Reward: -5.800000000000002\n",
      "Experience: 15----- Reward: 5.5\n",
      "Experience: 16----- Reward: 3.700000000000003\n",
      "Experience: 17----- Reward: -21.299999999999997\n",
      "Experience: 18----- Reward: -23.000000000000007\n",
      "Experience: 19----- Reward: -23.000000000000007\n",
      "Experience: 20----- Reward: -23.000000000000007\n",
      "Experience: 21----- Reward: -5.699999999999996\n",
      "Experience: 22----- Reward: -24.000000000000007\n",
      "Experience: 23----- Reward: -23.000000000000007\n",
      "Experience: 24----- Reward: 6.199999999999999\n",
      "Experience: 25----- Reward: -24.000000000000007\n",
      "Experience: 26----- Reward: 6.3\n",
      "Experience: 27----- Reward: 6.4\n",
      "Experience: 28----- Reward: -23.000000000000007\n",
      "Experience: 29----- Reward: -13.7\n",
      "Experience: 30----- Reward: -5.499999999999995\n",
      "The total time elapsed in seconds is: 325.02102494239807\n",
      "Mean Total Reward: -13.947, StdErr Total Reward: 2.126"
     ]
    }
   ],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "start = time()\n",
    "\n",
    "method = \"baseline_bumper\"\n",
    "df = DataFrame(num_experience = Int[], reward = String[])\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "exps = 30\n",
    "\n",
    "for exp = 1:exps    \n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = ToEnd(0,0,0,0,0)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater, max_steps=150)])\n",
    "    \n",
    "    println(\"Experience: \", string(exp), \"----- Reward: \", traj_rewards)\n",
    "    push!(total_rewards, traj_rewards)\n",
    "    \n",
    "    # Save in a dataframe to finally pull results in a .csv file\n",
    "    push!(df, (exp, string(traj_rewards)))\n",
    "    CSV.write(string(method, \".csv\"), df)\n",
    "end\n",
    "\n",
    "elapsed = time() - start\n",
    "\n",
    "print(\"The total time elapsed in seconds is: \", elapsed, \"\\n\")\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(exps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hyperparameter analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Influence of uncertainty in the problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### i) v_noise_coefficient = 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Recompiling stale cache file /Users/cbartolm/.julia/compiled/v1.0/CSV/HHBkp.ji for CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience: 1 ----- Reward: -10.400000000000004\n",
      "Experience: 2 ----- Reward: -1.7999999999999972\n",
      "Experience: 3 ----- Reward: -15.999999999999998\n",
      "Experience: 4 ----- Reward: -20.6\n",
      "Experience: 5 ----- Reward: -24.000000000000007\n",
      "Experience: 6 ----- Reward: -23.599999999999998\n",
      "Experience: 7 ----- Reward: -30.1\n",
      "Experience: 8 ----- Reward: -9.500000000000016\n",
      "Experience: 9 ----- Reward: -18.1\n",
      "Experience: 10 ----- Reward: -3.899999999999997\n",
      "Experience: 11 ----- Reward: -25.000000000000007\n",
      "Experience: 12 ----- Reward: -23.00000000000001\n",
      "Experience: 13 ----- Reward: -25.499999999999993\n",
      "Experience: 14 ----- Reward: -5.800000000000002\n",
      "Experience: 15 ----- Reward: 5.5\n",
      "Experience: 16 ----- Reward: 3.700000000000003\n",
      "Experience: 17 ----- Reward: -21.299999999999997\n",
      "Experience: 18 ----- Reward: -23.000000000000007\n",
      "Experience: 19 ----- Reward: -23.000000000000007\n",
      "Experience: 20 ----- Reward: -23.000000000000007\n",
      "Experience: 21 ----- Reward: -5.699999999999996\n",
      "Experience: 22 ----- Reward: -24.000000000000007\n",
      "Experience: 23 ----- Reward: -23.000000000000007\n",
      "Experience: 24 ----- Reward: 6.199999999999999\n",
      "Experience: 25 ----- Reward: -24.000000000000007\n",
      "Experience: 26 ----- Reward: 6.3\n",
      "Experience: 27 ----- Reward: 6.4\n",
      "Experience: 28 ----- Reward: -23.000000000000007\n",
      "Experience: 29 ----- Reward: -13.7\n",
      "Experience: 30 ----- Reward: -5.499999999999995\n",
      "The total execution time of the 30 experiments is: 536.3762290477753\n",
      "Mean Total Reward: -13.947, StdErr Total Reward: 2.126"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "v_noise_coefficient_2 = 8.0\n",
    "om_noise_coefficient_2 = 0.5\n",
    "\n",
    "belief_updater_2 = RoombaParticleFilter(spf, v_noise_coefficient_2, om_noise_coefficient_2);\n",
    "\n",
    "start = time()\n",
    "\n",
    "method = \"baseline_bumper_v8.0\"\n",
    "df = DataFrame(num_experience = Int[], reward = String[])\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "exps = 30\n",
    "\n",
    "for exp = 1:exps    \n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = ToEnd(0,0,0,0,0)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater_2, max_steps=150)])\n",
    "    \n",
    "    println(\"Experience: \", string(exp), \" ----- Reward: \", traj_rewards)\n",
    "    push!(total_rewards, traj_rewards)\n",
    "    \n",
    "    # Save in a dataframe to finally pull results in a .csv file\n",
    "    push!(df, (exp, string(traj_rewards)))\n",
    "    CSV.write(string(method, \".csv\"), df)\n",
    "end\n",
    "\n",
    "total_time = time() - start\n",
    "\n",
    "print(\"The total execution time of the 30 experiments is: \", total_time, \"\\n\")\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(exps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ii) v_noise_coefficient = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience: 1 ----- Reward: -10.400000000000004\n",
      "Experience: 2 ----- Reward: -1.7999999999999972\n",
      "Experience: 3 ----- Reward: -15.999999999999998\n",
      "Experience: 4 ----- Reward: -20.6\n",
      "Experience: 5 ----- Reward: -24.000000000000007\n",
      "Experience: 6 ----- Reward: -23.599999999999998\n",
      "Experience: 7 ----- Reward: -30.1\n",
      "Experience: 8 ----- Reward: -9.500000000000016\n",
      "Experience: 9 ----- Reward: -18.1\n",
      "Experience: 10 ----- Reward: -3.899999999999997\n",
      "Experience: 11 ----- Reward: -25.000000000000007\n",
      "Experience: 12 ----- Reward: -23.00000000000001\n",
      "Experience: 13 ----- Reward: -25.499999999999993\n",
      "Experience: 14 ----- Reward: -5.800000000000002\n",
      "Experience: 15 ----- Reward: 5.5\n",
      "Experience: 16 ----- Reward: 3.700000000000003\n",
      "Experience: 17 ----- Reward: -21.299999999999997\n",
      "Experience: 18 ----- Reward: -23.000000000000007\n",
      "Experience: 19 ----- Reward: -23.000000000000007\n",
      "Experience: 20 ----- Reward: -23.000000000000007\n",
      "Experience: 21 ----- Reward: -5.699999999999996\n",
      "Experience: 22 ----- Reward: -24.000000000000007\n",
      "Experience: 23 ----- Reward: -23.000000000000007\n",
      "Experience: 24 ----- Reward: 6.199999999999999\n",
      "Experience: 25 ----- Reward: -24.000000000000007\n",
      "Experience: 26 ----- Reward: 6.3\n",
      "Experience: 27 ----- Reward: 6.4\n",
      "Experience: 28 ----- Reward: -23.000000000000007\n",
      "Experience: 29 ----- Reward: -13.7\n",
      "Experience: 30 ----- Reward: -5.499999999999995\n",
      "The total execution time of the 30 experiments is: 717.729453086853\n",
      "Mean Total Reward: -13.947, StdErr Total Reward: 2.126"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "v_noise_coefficient_3 = 0.5\n",
    "om_noise_coefficient_3 = 0.5\n",
    "\n",
    "belief_updater_3 = RoombaParticleFilter(spf, v_noise_coefficient_3, om_noise_coefficient_3);\n",
    "\n",
    "start = time()\n",
    "\n",
    "method = \"baseline_bumper_v0.5\"\n",
    "df = DataFrame(num_experience = Int[], reward = String[])\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "exps = 30\n",
    "\n",
    "for exp = 1:exps    \n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = ToEnd(0,0,0,0,0)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater_3, max_steps=150)])\n",
    "    \n",
    "    println(\"Experience: \", string(exp), \" ----- Reward: \", traj_rewards)\n",
    "    push!(total_rewards, traj_rewards)\n",
    "    \n",
    "    # Save in a dataframe to finally pull results in a .csv file\n",
    "    push!(df, (exp, string(traj_rewards)))\n",
    "    CSV.write(string(method, \".csv\"), df)\n",
    "end\n",
    "\n",
    "total_time = time() - start\n",
    "\n",
    "print(\"The total execution time of the 30 experiments is: \", total_time, \"\\n\")\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(exps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### iii) om_noise_coefficient = 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience: 1 ----- Reward: -10.400000000000004\n",
      "Experience: 2 ----- Reward: -1.7999999999999972\n",
      "Experience: 3 ----- Reward: -15.999999999999998\n",
      "Experience: 4 ----- Reward: -20.6\n",
      "Experience: 5 ----- Reward: -24.000000000000007\n",
      "Experience: 6 ----- Reward: -23.599999999999998\n",
      "Experience: 7 ----- Reward: -30.1\n",
      "Experience: 8 ----- Reward: -9.500000000000016\n",
      "Experience: 9 ----- Reward: -18.1\n",
      "Experience: 10 ----- Reward: -3.899999999999997\n",
      "Experience: 11 ----- Reward: -25.000000000000007\n",
      "Experience: 12 ----- Reward: -23.00000000000001\n",
      "Experience: 13 ----- Reward: -25.499999999999993\n",
      "Experience: 14 ----- Reward: -5.800000000000002\n",
      "Experience: 15 ----- Reward: 5.5\n",
      "Experience: 16 ----- Reward: 3.700000000000003\n",
      "Experience: 17 ----- Reward: -21.299999999999997\n",
      "Experience: 18 ----- Reward: -23.000000000000007\n",
      "Experience: 19 ----- Reward: -23.000000000000007\n",
      "Experience: 20 ----- Reward: -23.000000000000007\n",
      "Experience: 21 ----- Reward: -5.699999999999996\n",
      "Experience: 22 ----- Reward: -24.000000000000007\n",
      "Experience: 23 ----- Reward: -23.000000000000007\n",
      "Experience: 24 ----- Reward: 6.199999999999999\n",
      "Experience: 25 ----- Reward: -24.000000000000007\n",
      "Experience: 26 ----- Reward: 6.3\n",
      "Experience: 27 ----- Reward: 6.4\n",
      "Experience: 28 ----- Reward: -23.000000000000007\n",
      "Experience: 29 ----- Reward: -13.7\n",
      "Experience: 30 ----- Reward: -5.499999999999995\n",
      "The total execution time of the 30 experiments is: 311.95691204071045\n",
      "Mean Total Reward: -13.947, StdErr Total Reward: 2.126"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "v_noise_coefficient_4 = 2\n",
    "om_noise_coefficient_4 = 2.5\n",
    "\n",
    "belief_updater_4 = RoombaParticleFilter(spf, v_noise_coefficient_4, om_noise_coefficient_4);\n",
    "\n",
    "start = time()\n",
    "\n",
    "method = \"baseline_bumper_om2.5\"\n",
    "df = DataFrame(num_experience = Int[], reward = String[])\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "exps = 30\n",
    "\n",
    "for exp = 1:exps    \n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = ToEnd(0,0,0,0,0)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater_4, max_steps=150)])\n",
    "    \n",
    "    println(\"Experience: \", string(exp), \" ----- Reward: \", traj_rewards)\n",
    "    push!(total_rewards, traj_rewards)\n",
    "    \n",
    "    # Save in a dataframe to finally pull results in a .csv file\n",
    "    push!(df, (exp, string(traj_rewards)))\n",
    "    CSV.write(string(method, \".csv\"), df)\n",
    "end\n",
    "\n",
    "total_time = time() - start\n",
    "\n",
    "print(\"The total execution time of the 30 experiments is: \", total_time, \"\\n\")\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(exps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### iv) om_noise_coefficient = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experience: 1 ----- Reward: -10.400000000000004\n",
      "Experience: 2 ----- Reward: -1.7999999999999972\n",
      "Experience: 3 ----- Reward: -15.999999999999998\n",
      "Experience: 4 ----- Reward: -20.6\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] transition(::RoombaPOMDP{Bumper,Bool}, ::RoombaState, ::StaticArrays.SArray{Tuple{2},Float64,1,2}) at /Users/cbartolm/Desktop/Projects/CS238_Project/src/roomba_env.jl:255",
      " [2] macro expansion at /Users/cbartolm/.julia/packages/POMDPs/JiYXY/src/generative_impl.jl:19 [inlined]",
      " [3] generate_s(::RoombaPOMDP{Bumper,Bool}, ::RoombaState, ::StaticArrays.SArray{Tuple{2},Float64,1,2}, ::MersenneTwister) at /Users/cbartolm/.julia/packages/POMDPs/JiYXY/src/generative_impl.jl:18",
      " [4] update(::RoombaParticleFilter, ::ParticleCollection{RoombaState}, ::RoombaAct, ::Bool) at /Users/cbartolm/Desktop/Projects/CS238_Project/src/filtering.jl:81",
      " [5] update_info at /Users/cbartolm/.julia/packages/POMDPModelTools/eHEjm/src/info.jl:52 [inlined]",
      " [6] iterate(::POMDPSimulators.POMDPSimIterator{(:s, :a, :r, :sp, :t, :i, :ai, :b, :o, :bp, :ui),RoombaPOMDP{Bumper,Bool},ToEnd,RoombaParticleFilter,MersenneTwister,ParticleCollection{RoombaState},RoombaState}, ::Tuple{Int64,RoombaState,ParticleCollection{RoombaState}}) at /Users/cbartolm/.julia/packages/POMDPSimulators/xyfJM/src/stepthrough.jl:104",
      " [7] iterate at ./generator.jl:44 [inlined]",
      " [8] grow_to!(::Array{Float64,1}, ::Base.Generator{POMDPSimulators.POMDPSimIterator{(:s, :a, :r, :sp, :t, :i, :ai, :b, :o, :bp, :ui),RoombaPOMDP{Bumper,Bool},ToEnd,RoombaParticleFilter,MersenneTwister,ParticleCollection{RoombaState},RoombaState},getfield(Main, Symbol(\"##9#10\"))}, ::Tuple{Int64,RoombaState,ParticleCollection{RoombaState}}) at ./array.jl:700",
      " [9] grow_to!(::Array{Any,1}, ::Base.Generator{POMDPSimulators.POMDPSimIterator{(:s, :a, :r, :sp, :t, :i, :ai, :b, :o, :bp, :ui),RoombaPOMDP{Bumper,Bool},ToEnd,RoombaParticleFilter,MersenneTwister,ParticleCollection{RoombaState},RoombaState},getfield(Main, Symbol(\"##9#10\"))}) at ./array.jl:678",
      " [10] collect(::Base.Generator{POMDPSimulators.POMDPSimIterator{(:s, :a, :r, :sp, :t, :i, :ai, :b, :o, :bp, :ui),RoombaPOMDP{Bumper,Bool},ToEnd,RoombaParticleFilter,MersenneTwister,ParticleCollection{RoombaState},RoombaState},getfield(Main, Symbol(\"##9#10\"))}) at ./array.jl:617",
      " [11] top-level scope at In[9]:24"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "using DataFrames\n",
    "using CSV\n",
    "\n",
    "v_noise_coefficient_5 = 2\n",
    "om_noise_coefficient_5 = 0.2\n",
    "\n",
    "belief_updater_5 = RoombaParticleFilter(spf, v_noise_coefficient_5, om_noise_coefficient_5);\n",
    "\n",
    "start = time()\n",
    "\n",
    "method = \"baseline_bumper_om0.2\"\n",
    "df = DataFrame(num_experience = Int[], reward = String[])\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "exps = 30\n",
    "\n",
    "for exp = 1:exps    \n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = ToEnd(0,0,0,0,0)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater_5, max_steps=150)])\n",
    "    \n",
    "    println(\"Experience: \", string(exp), \" ----- Reward: \", traj_rewards)\n",
    "    push!(total_rewards, traj_rewards)\n",
    "    \n",
    "    # Save in a dataframe to finally pull results in a .csv file\n",
    "    push!(df, (exp, string(traj_rewards)))\n",
    "    CSV.write(string(method, \".csv\"), df)\n",
    "end\n",
    "\n",
    "total_time = time() - start\n",
    "\n",
    "print(\"The total execution time of the 30 experiments is: \", total_time, \"\\n\")\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(exps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same values! For the bumper baseline model, uncertainty in v makes no difference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "7eaf0b86a4eb4948809c5b321cc52473",
   "lastKernelId": "62631fb2-5f62-46b5-9e1d-be41590a8238"
  },
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
